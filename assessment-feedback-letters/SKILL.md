---
name: assessment-feedback-letters
description: Generate individual feedback letters for department chairs regarding their Annual Assessment Reports. Uses a consistent structure and warm, collegial tone to provide constructive feedback on assessment practices in higher education.
---

# Assessment Feedback Letter Generator

This skill helps draft complete feedback letters to department chairs about their Annual Assessment Reports submitted at the end of the academic year. These letters provide encouragement, identify strengths, note areas for improvement, and offer actionable recommendations.

## When to Use This Skill

Use this skill when:
- Creating feedback letters for department Annual Assessment Reports
- The user provides summary points and/or key observations (not full reports)
- The goal is a complete, ready-to-send letter
- The context is academic program assessment in higher education

## Letter Structure

Follow this exact structure for every feedback letter:

### 1. Subject Line
Format: "[Department Name]'s Annual Assessment Report – Feedback"

### 2. Salutation
Use the chair's first name: "Dear [Name],"

### 3. Opening Paragraph
- Thank the chair for submitting the report
- Acknowledge the work and thoughtfulness
- Preview the feedback structure
- Maintain warmth and appreciation
- Example: "Thank you for submitting the [Department]'s AY 2024–25 Annual Assessment Report. I appreciated the thoughtful work you and your colleagues did this past year. Below are some of my general observations that I want to share as well as a few targeted recommendations for approaching next year's assessment work. As always, these are just recommendations and there is nothing further that needs to be done for this year's Annual Assessment Report."

### 4. Observed Strengths Section
**Heading:** "Observed strengths:"
- Use bullet points (•)
- Highlight 3-6 specific positive aspects
- Include concrete details about what worked well
- Recognize good assessment practices
- Be specific rather than generic (e.g., "The use of both first-day and last-day quizzes helped illustrate growth over time" rather than "Good assessment design")

### 5. Observed Areas for Improvement Section
**Heading:** "Observed areas for improvement:"
- Use bullet points (•)
- Provide 2-4 constructive observations
- Keep tone developmental, not critical
- Be specific and balanced
- Focus on what's missing or could be strengthened
- Common themes to watch for:
  - Missing references to previous SLO assessments
  - Lack of "closing the loop" documentation
  - Vague or overly general action plans
  - Missing details about implementation or responsibility
  - Benchmarks not met without sufficient discussion

### 6. Recommendations Section
**Heading:** "Recommendations to consider for AY [Next Year]:"
- Use bullet points (•)
- Provide 2-5 forward-looking, actionable suggestions
- Frame as optional improvements, not requirements
- Use conditional language: "You might consider...", "I'd encourage...", "A possible next step could be..."
- **CRITICAL:** Every item in this section must be an actionable recommendation, not an observation or restatement of strengths
- Test each bullet: Can the department DO something specific based on this? If not, it doesn't belong here
- Bad example: "The benchmark was clearly defined and students performed well" (This is an observation)
- Good example: "Consider including one or two anonymized student responses in future reports to illustrate how students engage with the material" (This is actionable)

### 7. Detailed Elaboration (When Appropriate)
- For complex assessments, provide more detailed commentary within the recommendations section
- Structure: Brief observation + specific recommendation
- May include references to specific courses, rubrics, or data
- Keep closely tied to actionable next steps

### 8. Closing Paragraph
Standard closing that:
- Thanks again for the work
- Expresses appreciation for ongoing efforts
- Offers to meet and discuss
- Maintains warm, supportive tone
- **Standard text:** "Thank you again for the time and care your department put into this report. I appreciate the ongoing work you're doing to reflect on student learning and to strengthen your program. If it would be helpful to meet and talk through any part of this feedback, or to discuss your assessment plans or anything else that's on your mind, I'd be glad to set up a time."

### 9. Sign-off
Always use: "Take care, Rob"

### 10. Appendix (If Applicable)
If a rubric table or summary is provided, include it at the end with a clear heading like:
"Rubric Summary for SLO [#] – [Topic]"

## Content Guidelines

### Tone and Voice
- Warm, collegial, and encouraging without being saccharine
- Sound like a thoughtful peer, not a supervisor or evaluator
- Respectful of faculty autonomy and expertise
- Balanced: acknowledge good work while offering constructive guidance
- Use Rob's professional communications style (refer to rob-professional-communications skill)

### Language Conventions
- **Never use em dashes (—)**
- Use conditional phrasing: "I might suggest...", "You might consider...", "I'd encourage..."
- Avoid corporate jargon
- Be specific and concrete rather than abstract
- Use "what works well" and "what could be strengthened" framing

### Common Assessment Themes to Address

**Positive indicators:**
- Direct measures aligned with SLOs
- Clear benchmarks appropriate for the level
- Use of rubrics with norming/calibration
- Evidence of inter-rater reliability
- Comparison to previous assessment cycles
- Clear documentation of changes made
- Meaningful interpretation of results
- Honest reflection on gaps or challenges

**Areas typically needing improvement:**
- Missing reference to previous assessment of the SLO
- No documentation of "closing the loop" (actions taken since last assessment)
- Benchmarks met without discussion of outliers or opportunities
- Action plans that are vague or philosophical rather than concrete
- No mention of who will implement changes or when
- Insufficient detail about assignments, prompts, or rubrics
- Limited evidence of how results are shared with students

### Recommendations to Commonly Suggest

When appropriate, consider recommending:
- Reference previous assessment cycles and document changes made
- Include specific details about implementation (who, when, how)
- Add anonymized student work examples to illustrate performance levels
- Clarify benchmarks or add nuance (e.g., separate by rater, by course level)
- Consider additional or complementary measures
- Build in formative assessments alongside summative ones
- Establish clear responsibility for monitoring ongoing results
- Include timeline for reassessment
- Share how results are communicated to students
- Address patterns in student performance even when benchmarks are met

## Quality Checks

Before finalizing each letter:

1. **Structure check:** Does it follow all 9-10 sections in order?
2. **Actionability check:** Are all items under "Recommendations" truly actionable?
3. **Balance check:** Is the tone encouraging yet constructive?
4. **Specificity check:** Are observations concrete and tied to actual assessment practices?
5. **Voice check:** Does it sound like Rob (warm, collegial, conditional phrasing)?
6. **Consistency check:** Is the department name and chair name correct throughout?
7. **Em dash check:** No em dashes (—) anywhere in the letter?

## Input Requirements

To generate a letter, the user should provide:
1. **Department name** and **department chair's first name**
2. **SLO(s) reviewed** (with the SLO text if available)
3. **Key observations summary** covering:
   - What was assessed (measures, assignments, courses)
   - Results and benchmarks
   - Strengths observed
   - Areas for improvement
   - Any action plans mentioned
4. **Rubric table or summary** (if applicable for appendix)

## Examples of Section Balance

### Example: Appropriate Recommendations Section
**Good:**
- When you reassess this SLO, include a one-line reference to the last time it was assessed and what changed since then.
- Consider adding a brief discussion of the one student who fell below the benchmark to identify what might be learned from that case.
- You might want to establish a timeline for reassessment of each SLO, even when no changes are planned.

**Bad (these are observations, not recommendations):**
- The rubric was clearly designed and aligned with the outcome.
- Students performed well, with 85% meeting the benchmark.
- The assessment used a strong direct measure.

### Example: Moving Observations to Strengths
If something describes what was done well, it belongs in "Observed strengths," not "Recommendations."

**Wrong placement (in Recommendations):**
- The department used a multi-course design that gave a clear sense of developmental progress.

**Correct placement (in Observed strengths):**
- The multi-course design gave a clear sense of developmental progress across 100, 200, and 400-level classes.

## Integration with Other Skills

This skill should be used in conjunction with:
- **rob-professional-communications**: For overall tone and style
- Future skills for initial evaluation (when created)

## Notes

- These letters are sent via email, so they should be formatted as email-ready text
- The goal is encouragement and support, not compliance or criticism
- Assessment is an ongoing process of improvement, not a one-time judgment
- Respect the professional autonomy of faculty while offering helpful guidance
