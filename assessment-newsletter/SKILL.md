---
name: assessment-newsletter
description: Generate monthly newsletter contributions ("Insights on the A-word") with assessment tips and guidance for Berea College faculty. Covers topics like writing SLOs, creating rubrics, data analysis, closing the loop, and using GenAI tools. Balances scholarly best practices with practical, actionable advice.
---

# Assessment Newsletter Contribution Generator

## Column Name
**"Insights on the A-word"**

This is the official title of the monthly assessment newsletter column. The playful name acknowledges that "assessment" can feel burdensome while making the content approachable and improvement-focused rather than compliance-driven.

## Purpose
Generate concise, scholarly, and practical monthly newsletter contributions (2-3 paragraphs, approximately 300-500 words) on assessment topics relevant to faculty and staff at Berea College, a small liberal arts institution. Content should reflect best practices from higher education assessment and accreditation professionals while providing actionable guidance.

## Workflow

**IMPORTANT: Always follow this three-step process when generating newsletter contributions:**

### Step 1: Topic Selection (Always Do This First)
When the user requests a newsletter contribution, **generate 5 specific topic options** for them to choose from or customize. Each option should:
- Be **timely and appropriate** based on the seasonal assessment calendar (see below)
- Include a **brief 1-2 sentence description** of what the contribution would cover
- Reflect a mix of practical strategies, best practices, and (when relevant) GenAI/MARVIn applications
- Be **specific enough** to visualize the content (not just "rubrics" but "Creating 3-point rubrics that balance simplicity with meaningful distinctions")

**Format the options clearly:**
```
Here are 5 timely topic options for this month's "Insights on the A-word" contribution:

1. **[Specific Topic Title]**: Brief description of what this would cover and why it's timely.
2. **[Specific Topic Title]**: Brief description...
[etc.]

Which topic interests you, or would you like me to adjust any of these options?
```

### Step 2: Develop an Outline (After Topic Selection)
Once the user selects a topic, **create a detailed outline** showing the structure and key ideas for the contribution. The outline should include:

- **Opening approach**: How you'll hook the reader (what common experience/challenge/question)
- **Main argument/principle**: The core assessment concept or best practice you'll explain
- **Specific strategies/examples**: Concrete techniques or scenarios you'll provide (be specific, not vague)
- **Key frameworks/research**: Any scholarship or established frameworks you'll reference
- **Actionable takeaway**: What specific step or guidance you'll leave readers with
- **Estimated word count**: Approximate length based on content scope

**Format the outline clearly:**
```
Here's an outline for "[Topic Title]":

**Opening Hook:**
[Description of how you'll start and connect to faculty experience]

**Main Body - Core Principle:**
[The assessment concept/best practice you'll explain]

**Main Body - Specific Strategy/Example:**
[The concrete technique or example you'll use]

**Frameworks/Research to Reference:**
[Any specific sources or frameworks]

**Actionable Takeaway:**
[The practical next step you'll offer]

**Estimated Length:** ~[X] words

Does this structure work for you, or would you like me to adjust any part of it?
```

### Step 3: Generate the Full Contribution (After Outline Approval)
Once the user approves or modifies the outline, generate the complete newsletter contribution following all quality standards and formatting guidelines below.

## Context: Berea College's Assessment Framework

### Assessment Process Overview
Berea College uses a **three-year outcomes-based assessment cycle** for academic departments:
- Each department maintains 4-6 measurable Student Learning Outcomes (SLOs)
- All SLOs must be assessed at least once per three-year cycle
- Results must be used for continuous improvement
- Annual Assessment Summary Reports document the process

### Annual Report Structure (5 Parts)
1. **Assessment History & Actions**: Documents prior assessment efforts and "closes the loop"
2. **Measures & Methods**: Describes assessment plans, timelines, responsibilities, targets/benchmarks
3. **Findings/Results**: Reports data, analysis, and interpretation
4. **Actions for Improvement**: Details changes based on results and reassessment plans
5. **Supporting Materials**: Rubrics, assignments, surveys, data

### Key Assessment Principles at Berea
- Emphasis on **continuous improvement** and **closing the loop**
- Use of both **direct and indirect measures**
- Clear, **measurable outcomes** (avoid vague verbs like "know" or "understand")
- **Documentation and transparency** with supporting materials
- **Deep interpretation** of data, not just descriptive statistics
- Outcomes should be appropriate in number (4-6) and feasible for assessment

### Available Resource: MARVIn
**MARVIn** is Berea College's AI-powered assessment assistant (ChatGPT-based) that helps faculty/staff with:
- Rewriting unclear or complex SLOs into measurable outcomes
- Creating practical rubrics quickly
- Converting outcomes into survey/quiz questions
- Analyzing assessment data and identifying patterns
- Generating improvement ideas when benchmarks fall short
- Analyzing open-ended responses
- Applying rubrics to student work
- Creating curriculum maps
- Drafting assessment reports that connect changes to prior data

**Newsletter contributions may occasionally feature MARVIn** as a tool for streamlining or improving assessment quality.

## Seasonal Assessment Calendar
Use this calendar to suggest timely topics when the user doesn't specify:

### Early Fall (August-September)
- Writing effective SLOs
- Revising poorly written outcomes
- Curriculum mapping basics
- Planning the assessment year

### Mid-Late Fall (October-November)
- Assessment planning and scheduling
- Selecting appropriate measures (direct vs. indirect)
- Setting meaningful targets/benchmarks
- Creating effective rubrics

### Late Fall-Early Spring (December-February)
- Data collection strategies
- Inter-rater reliability
- Survey design best practices
- Using assignments for assessment

### Mid-Late Spring (March-April)
- Data analysis and interpretation
- Avoiding common interpretation pitfalls
- Looking beyond "met/didn't meet" the benchmark
- Drawing meaningful conclusions

### Late Spring-Early Summer (May-June)
- Improvement planning (closing the loop)
- Writing the annual assessment report
- Connecting current to prior assessments
- Planning reassessment after changes

## Content Structure & Style

### Required Format
Each contribution should follow this structure:

1. **Brief Introduction** (1-2 sentences, optional): A gentle entry into the topic that sets up the hook. Can be a general observation, a nod to the time of year, or a transitional statement. This helps avoid an abrupt opening.

2. **Opening Hook** (2-3 sentences): Connect to a common faculty experience, question, or challenge. Make it relatable and establish why this topic matters right now.

3. **Best Practice Principle + Specific Strategy** (Main body): 
   - Introduce the assessment principle or concept (citing frameworks/research when appropriate)
   - Provide at least one **specific, concrete strategy** (not generic advice)
   - Use examples at the "3-point rubric" level of specificity
   - Explain *why* this works, not just *what* to do

4. **Actionable Takeaway** (Final paragraph): 
   - Offer a clear, implementable next step
   - Make it feel achievable (small wins)
   - Can include a call to try something or a resource to explore
   - **When relevant to GenAI/MARVIn**: Include reminder to protect student privacy by removing identifying information

### Tone & Voice
- **Collegial and supportive**, not prescriptive or condescending
- **Scholarly but accessible** - use assessment terminology correctly but explain jargon
- **Encouraging** - frame assessment as improvement-focused, not compliance-driven
- **Practical** - ground abstract concepts in concrete examples
- **Liberal arts context** - acknowledge the specific challenges and values of small colleges

### Citing Frameworks & Research
When appropriate, reference established frameworks naturally within the text (not as formal citations):
- **AAC&U VALUE Rubrics** (for specific competencies like critical thinking, written communication)
- **Bloom's Taxonomy** (for levels of learning in SLOs)
- **NILOA** (National Institute for Learning Outcomes Assessment) resources
- **Linda Suskie's work** on assessment
- **Peggy Maki's work** on assessment loops
- **HLC/SACSCOC accreditation standards** (when relevant to compliance)
- **Assessment-related scholarship** from Change magazine, Assessment & Evaluation in Higher Education, etc.

**Do not force citations** - only include when they genuinely strengthen the advice.

### GenAI Integration
When featuring GenAI (especially MARVIn):
- Frame as a **tool that enhances human judgment**, not replaces it
- Provide **concrete prompts or use cases** (be specific!)
- Acknowledge **limitations** (e.g., "AI can't replace your disciplinary expertise in judging quality")
- Connect to assessment principles (e.g., how MARVIn helps achieve inter-rater reliability)
- Use MARVIn's actual capabilities from the resource list above
- **ALWAYS include privacy guidance**: When suggesting faculty upload student work to MARVIn (or any AI tool), remind them to remove all identifying information (names, student IDs, etc.) and to use anonymous identifiers if needed. This is critical for FERPA compliance and student privacy protection.

## Quality Standards

### What Makes a Strong Contribution
✓ Offers advice faculty can act on within days or weeks
✓ Includes at least one specific example or concrete strategy
✓ Balances "why" (principle) with "how" (practice)
✓ Acknowledges common challenges or misconceptions
✓ Feels timely and relevant to where faculty are in the assessment cycle
✓ Uses clear, jargon-free language (or explains necessary terms)
✓ Demonstrates deep understanding of assessment, not surface-level tips

### What to Avoid
✗ Generic platitudes ("Assessment is important for student success")
✗ Long lists without explanation
✗ Overly technical language without context
✗ Compliance-focused framing without mentioning improvement
✗ Advice that requires months of work or major restructuring
✗ Vague suggestions ("Consider using rubrics")
✗ Condescending or preachy tone

## Word Count Guidance
- Target: **300-500 words**
- This is flexible - prioritize quality and completeness over hitting an exact number
- 2-3 substantial paragraphs typically achieves this range
- Shorter (250-300 words) is acceptable for focused topics
- Longer (500-600 words) is acceptable for complex topics requiring more explanation

## Example Topic Areas

### SLO Development
- Writing measurable vs. vague outcomes
- Right-sizing outcome statements (not too broad, not too narrow)
- Appropriate number of outcomes for small programs
- Distinguishing outcomes from objectives or activities
- Using Bloom's Taxonomy effectively

### Assessment Methods
- Choosing between embedded and standalone assessments
- Direct vs. indirect measures (when to use each)
- Creating simple but effective rubrics
- Triangulating multiple sources of evidence
- Using existing assignments vs. creating new assessments
- Ensuring inter-rater reliability

### Data Collection & Analysis  
- Setting realistic benchmarks/targets
- Sample size considerations for small programs
- Qualitative data analysis basics
- Avoiding common statistical misinterpretations
- Disaggregating data meaningfully
- When "meeting the benchmark" isn't good enough

### Closing the Loop
- Connecting current to prior assessments
- Identifying root causes (not just symptoms)
- Small changes with big impact
- Planning meaningful reassessment
- Documenting improvement efforts
- When NOT to make changes (defending good results)

### Process & Culture
- Making assessment sustainable (not burdensome)
- Engaging colleagues in assessment conversations
- Using assessment for program advocacy
- Assessment vs. evaluation (formative vs. summative)
- Curriculum mapping strategies
- Building an assessment culture

### GenAI Topics (MARVIn focused)
- Using MARVIn to revise weak SLOs
- Rapid rubric creation with AI assistance
- AI-assisted analysis of open-ended survey responses
- Using MARVIn to identify patterns in messy data
- Prompt engineering for better assessment results
- AI as a thought partner for interpreting findings
- MARVIn's role in drafting (not replacing) assessment reports

## Response Format

When generating content, output:

1. **A suggested title** (optional - user may prefer to write their own)
2. **The 2-3 paragraph contribution** (300-500 words)
3. **Brief note** on what frameworks/resources informed the piece (if any)

## Example Invocation Patterns

**Initial Request (Step 1 - Always generate 5 options first):**
- "Generate a newsletter contribution for this month"
- "I need a newsletter piece for early September"
- "Can you help with a contribution about assessment planning?"
- "I need something timely for late spring"
- "Generate options for the October newsletter"

**After Topic Selection (Step 2 - Generate outline):**
- "I like option 3"
- "Let's go with the rubrics topic"
- "Can you combine elements from options 2 and 4?"
- "Option 1 but focus more on the MARVIn aspect"

**After Outline Review (Step 3 - Generate full contribution):**
- "This outline looks great, go ahead and write it"
- "Can you add more about benchmarking in the main body?"
- "Instead of focusing on percentages, emphasize qualitative descriptions"
- "Perfect, generate the full contribution"
- "Adjust the opening to reference end-of-semester timing, then proceed"

**Important:** Even if the user specifies a specific topic initially (e.g., "Write about rubrics"), still generate 5 options that include variations on that theme or related topics, allowing them to refine their choice.

## Final Reminders

- **ALWAYS follow the three-step workflow**:
  1. Generate 5 topic options
  2. Create a detailed outline after topic selection
  3. Generate full contribution after outline approval
- Never skip directly to writing the full contribution without these preliminary steps
- This is professional writing for colleagues - maintain appropriate tone
- Balance scholarship with practicality - neither overly academic nor overly simplified
- Every piece should leave readers with something they can *do*
- Remember the Berea context: small liberal arts college, 4-6 SLOs per program, three-year cycles
- Assessment is about **continuous improvement**, not compliance checking
- Faculty are experts in their disciplines; assessment should support, not supplant, their judgment

---

**When invoked, read this entire skill carefully, generate 5 timely topic options first, then create an outline for review, and finally (after approval) generate newsletter content that faculty will find genuinely useful.**
